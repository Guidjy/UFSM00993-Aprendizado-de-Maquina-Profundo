{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#UFSM00993 - Aprendizado de Máquina Profundo T1\n",
        "####João Vitor Bernardi - 202410054\n",
        "####Guilherme Patrício Pimentel - 202412576\n",
        "---\n",
        "Implementação de um Multi-layer Perceptron (MLP) com uma camada oculta para a aproximação de funções binárias.\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "7LkJ_nK0kQhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "q5y1sOq2qbJx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Considere que apenas a camada oculta tem uma funcão de ativação (adote ReLU) ( ˜ i.e., a camada de saída tem\n",
        "ativação identidade). Otimize a MLP com uma função de perda que penaliza o quadrado do erro de estimação."
      ],
      "metadata": {
        "id": "HyXCu285pcqz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_GMNi0lGjQVx"
      },
      "outputs": [],
      "source": [
        "def Relu(n):\n",
        "    return np.maximum(0,n)\n",
        "\n",
        "def calculoLoss(saidaFinal, saidaCorreta):\n",
        "\n",
        "    return (saidaFinal - saidaCorreta)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Inicialize os coeficientes das matrizes de pesos Ω de tamanho N × M com os valores Ωi,j ∼ N (0,\n",
        "p\n",
        "2/N) (He\n",
        "initialization). Inicialize os coeficientes dos vetores de viéses β com um valor constante pequeno (e.g., βk = 0,1)."
      ],
      "metadata": {
        "id": "RJCFchEKp-i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculoParametro(pAntigo, taxaAprendizado, gradiente):\n",
        "\n",
        "    return pAntigo - taxaAprendizado * gradiente\n",
        "\n",
        "def heInitialization(n_entradas,n_saidas):\n",
        "\n",
        "    desvioPadrao = np.sqrt(2 / n_entradas)\n",
        "    saida = np.random.randn(n_entradas, n_saidas) * desvioPadrao\n",
        "    return saida"
      ],
      "metadata": {
        "id": "bvU6QrFmp66m"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Considere o método do gradiente descendente para otimização. Determine as derivadas da função de perda em ˜\n",
        "função de cada parâmetro e as implemente manualmente. Identifique os gradientes textualmente.\n"
      ],
      "metadata": {
        "id": "wk2b7sw8qIsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backWardPass(entradas, saidaCorreta, saidaFinal,pesosFinais, saidaOculta, saidaOcultaLinear):\n",
        "\n",
        "    # 1. Gradiente do viés da camada de saída (∇b2)\n",
        "    # É o erro da previsão, derivado da função de perda (erro quadrático).\n",
        "    gradienteB2 = 2*(saidaFinal - saidaCorreta)\n",
        "\n",
        "    # 2. Gradiente dos pesos da camada de saída (∇W2)\n",
        "    # É o erro da previsão, multiplicado pela saída da camada oculta.\n",
        "    # A transposição (.T) é para ajustar as dimensões da matriz.\n",
        "    saidaOculta_T = saidaOculta.T\n",
        "    gradienteW2 = saidaOculta_T @ gradienteB2\n",
        "\n",
        "    # 3. Gradiente do viés da camada oculta (∇b1)\n",
        "    # Erro propagado para a camada oculta.\n",
        "    pesosFinais_T = pesosFinais.T\n",
        "    derivada_relu = (saidaOcultaLinear > 0).astype(int)\n",
        "    gradienteB1 = (gradienteB2 @ pesosFinais_T) * derivada_relu # Multiplicação elemento a elemento\n",
        "\n",
        "    # 4. Gradiente dos pesos da camada oculta (∇W1)\n",
        "    # Erro da camada oculta, multiplicado pela entrada original.\n",
        "    entradas_T = entradas.T\n",
        "    gradienteW1 = entradas_T @ gradienteB1\n",
        "\n",
        "    return gradienteW2, gradienteB2, gradienteW1, gradienteB1"
      ],
      "metadata": {
        "id": "6P4gzB5CqFf3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Avalie experimentalmente o impacto de retreinar a MLP. Por fim, determine uma seed para os experimentos\n",
        "seguintes. Descreva os resultados.\n",
        "<br>\n",
        "5.a. Resultado do primeiro treino da MLP, que após 8000 épocas conseguiu prever de forma correta todas as entradas da XOR no teste final realizado\n"
      ],
      "metadata": {
        "id": "2qAsgGgHdnML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 8000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = np.random.rand(2, 2)\n",
        "pesosFinais = np.random.rand(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp-FjBYElun4",
        "outputId": "8be58239-2d74-4fd7-c14c-9be0eaf79d25"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.260730\n",
            "Época: 1000, loss médio: 0.259135\n",
            "Época: 1500, loss médio: 0.258011\n",
            "Época: 2000, loss médio: 0.257218\n",
            "Época: 2500, loss médio: 0.256688\n",
            "Época: 3000, loss médio: 0.256315\n",
            "Época: 3500, loss médio: 0.256041\n",
            "Época: 4000, loss médio: 0.255843\n",
            "Época: 4500, loss médio: 0.255694\n",
            "Época: 5000, loss médio: 0.255581\n",
            "Época: 5500, loss médio: 0.255493\n",
            "Época: 6000, loss médio: 0.255423\n",
            "Época: 6500, loss médio: 0.254089\n",
            "Época: 7000, loss médio: 0.169656\n",
            "Época: 7500, loss médio: 0.001387\n",
            "Época: 8000, loss médio: 0.000000\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.0002)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.9997)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.9997)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.0002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.b. Resultado do segundo treino da MLP, dessa vez com outros pesos iniciais escolhidos aleatoriamente pelo método da HeInitialization. Conseguiu aproximar em 2000 épocas."
      ],
      "metadata": {
        "id": "45POhyDBgVRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jff2v6LkgY45",
        "outputId": "a31361d4-a00b-4f3c-da68-37408f3fd4d8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.151750\n",
            "Época: 1000, loss médio: 0.103305\n",
            "Época: 1500, loss médio: 0.007572\n",
            "Época: 2000, loss médio: 0.000007\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.0033)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.9959)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.9992)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.0004)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.c Determinando a seed 25102005 para o processo da HeInitialization, os resultados da rede tornam-se constantes e previsíveis, já que o valor dos parâmetros iniciais sempre será o mesmo"
      ],
      "metadata": {
        "id": "-wygp-kI0WYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(25102005) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29wYSj570X7U",
        "outputId": "211940db-7a41-49dc-8cea-5f6960e20b99"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.134331\n",
            "Época: 1000, loss médio: 0.127551\n",
            "Época: 1500, loss médio: 0.127538\n",
            "Época: 2000, loss médio: 0.127538\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.0000)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.4949)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.9899)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.4949)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Avalie experimentalmente o impacto do numero de épocas e o valor da taxa de aprendizado. Descreva os\n",
        "resultados.\n",
        "\n",
        "6.a seed = 80, épocas = 1000, taxa de aprendizado = 0.01"
      ],
      "metadata": {
        "id": "JqIhnhBT0mct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 1000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umzZLPfa0q7F",
        "outputId": "99fdf5e7-60eb-4380-9c18-20782a590d30"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.151750\n",
            "Época: 1000, loss médio: 0.103305\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.4757)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.5812)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.9906)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (-0.0427)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com seed fixada no número 80, a Loss média do programa diminuiu ao longo de 1000 épocas, mas não o suficiente para gerar resultados mais precisos.\n",
        "\n",
        "6.b seed = 80, épocas = 5000, taxa de aprendizado = 0.01"
      ],
      "metadata": {
        "id": "cT5oGYP81LqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 5000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR-l1czC1WJJ",
        "outputId": "c62978c3-1af2-4b0a-f045-f38e6e7ad783"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.151750\n",
            "Época: 1000, loss médio: 0.103305\n",
            "Época: 1500, loss médio: 0.007572\n",
            "Época: 2000, loss médio: 0.000007\n",
            "Época: 2500, loss médio: 0.000000\n",
            "Época: 3000, loss médio: 0.000000\n",
            "Época: 3500, loss médio: 0.000000\n",
            "Época: 4000, loss médio: 0.000000\n",
            "Época: 4500, loss médio: 0.000000\n",
            "Época: 5000, loss médio: 0.000000\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.0000)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (1.0000)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (1.0000)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao alterar o número de épocas para 5000, mantendo a mesma seed, Loss médio diminuiu gradualmente ao longo do tempo até chegar a zero, resultando em respostas exatas nos testes finais.\n",
        "\n",
        "6.c seed = 80, épocas = 5000, taxa de aprendizado = 0.1"
      ],
      "metadata": {
        "id": "Wc8JlWvK1b4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 5000\n",
        "taxaAprendizado = 0.1\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNj9oQJV1hKR",
        "outputId": "4fac3d6d-0c6f-457d-e07e-15d3a902ba3c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.154321\n",
            "Época: 1000, loss médio: 0.154321\n",
            "Época: 1500, loss médio: 0.154321\n",
            "Época: 2000, loss médio: 0.154321\n",
            "Época: 2500, loss médio: 0.154321\n",
            "Época: 3000, loss médio: 0.154321\n",
            "Época: 3500, loss médio: 0.154321\n",
            "Época: 4000, loss médio: 0.154321\n",
            "Época: 4500, loss médio: 0.154321\n",
            "Época: 5000, loss médio: 0.154321\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.5556)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.5556)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (1.0000)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao aumentar o valor da taxa de aprendizado, o programa dá “saltos” muito grandes no processo da mudança de parâmetros, convergindo para um mínimo local e estagnando a Loss Média.\n",
        "\n",
        "6.d seed =80, épocas = 10000, taxa de aprendizado = 0.001"
      ],
      "metadata": {
        "id": "kUgjdekA1mLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 10000\n",
        "taxaAprendizado = 0.001\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln2sxO9T1qIr",
        "outputId": "21b32ac9-6044-4dc6-d461-c9098e423ba6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.207551\n",
            "Época: 1000, loss médio: 0.175352\n",
            "Época: 1500, loss médio: 0.163745\n",
            "Época: 2000, loss médio: 0.160753\n",
            "Época: 2500, loss médio: 0.159489\n",
            "Época: 3000, loss médio: 0.158435\n",
            "Época: 3500, loss médio: 0.157297\n",
            "Época: 4000, loss médio: 0.156017\n",
            "Época: 4500, loss médio: 0.154556\n",
            "Época: 5000, loss médio: 0.152890\n",
            "Época: 5500, loss médio: 0.150978\n",
            "Época: 6000, loss médio: 0.148775\n",
            "Época: 6500, loss médio: 0.146223\n",
            "Época: 7000, loss médio: 0.143257\n",
            "Época: 7500, loss médio: 0.139763\n",
            "Época: 8000, loss médio: 0.135659\n",
            "Época: 8500, loss médio: 0.130791\n",
            "Época: 9000, loss médio: 0.125000\n",
            "Época: 9500, loss médio: 0.118579\n",
            "Época: 10000, loss médio: 0.114401\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.4863)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.5314)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.9821)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (-0.0127)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Avalie trocar a função de ativação ReLU pelas funções de ativação softmax e identidade. Verifique se ha necessidade\n",
        "de mudar a forma de inicialização dos parâmetros da MLP (se sim, pesquisar por Glorot/Xavier initialization).\n",
        "Descreva os resultados.\n",
        "\n",
        "7.a Trocando ReLu por softmax"
      ],
      "metadata": {
        "id": "y8b5BiBB3XAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforma o vetor de saídas em probabilidades que somam 1\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # estabilidade numérica\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = softmax(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = softmax(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehUDMmmA3rPV",
        "outputId": "2cc2dc75-7838-462d-efa0-8709c8d97e73"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.258847\n",
            "Época: 1000, loss médio: 0.254597\n",
            "Época: 1500, loss médio: 0.247540\n",
            "Época: 2000, loss médio: 0.243798\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.5347)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.3762)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.6852)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.4076)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando usada em uma camada oculta, o Ssoftmax \"achata\" todos os valores, o que dificulta o aprendizado, pois todas as ativações dependem umas das outras. Como pode ser visto, a rede tende a parar de aprender muito rapidamente e a loss fica quase constante.\n",
        "\n",
        "7.b Trocando ReLu pela funçãi identidade"
      ],
      "metadata": {
        "id": "WbBOqPg34JVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identidade(x):\n",
        "    return x\n",
        "\n",
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = identidade(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = identidade(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-5PPP1O6cFM",
        "outputId": "0a89dee3-8f1e-48e9-e8b9-126ca330e9c9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.272424\n",
            "Época: 1000, loss médio: 0.268273\n",
            "Época: 1500, loss médio: 0.265282\n",
            "Época: 2000, loss médio: 0.263698\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.5137)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.4976)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.4825)\n",
            "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.4664)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função identidade não introduz não-linearidade, logo a rede se torna apenas uma grande regressão linear. Portanto, a rede não aprende XOR, e a loss diminui muito pouco.\n",
        "\n",
        "8. Avalie experimentalmente a aplicação da arquitetura padrão (considerando a função de ativação ReLU) para\n",
        "aproximar outras funções binárias. Considere, ao menos, AND e OR (ver tabela-verdade abaixo), definidas sobre\n",
        "numeros binários de forma análoga à função XOR. Descreva os resultados.\n",
        "\n",
        "8.a aproximação da função AND"
      ],
      "metadata": {
        "id": "w-i-1yIT6q0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6uEIO-67SKY",
        "outputId": "88e429ac-8d33-4379-d8fa-938d6b4b1f33"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.032069\n",
            "Época: 1000, loss médio: 0.000212\n",
            "Época: 1500, loss médio: 0.000000\n",
            "Época: 2000, loss médio: 0.000000\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (-0.0000)\n",
            "Entrada: [0 1], Saída Esperada: 0, Previsão da Rede: (0.0000)\n",
            "Entrada: [1 0], Saída Esperada: 0, Previsão da Rede: (0.0000)\n",
            "Entrada: [1 1], Saída Esperada: 1, Previsão da Rede: (1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.b Aproximação da função OR"
      ],
      "metadata": {
        "id": "VBpZwMJQ7xID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXtLv9nb7z7T",
        "outputId": "6dcc3d9f-209a-4dcb-c430-6a44ee928836"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.043887\n",
            "Época: 1000, loss médio: 0.020313\n",
            "Época: 1500, loss médio: 0.006428\n",
            "Época: 2000, loss médio: 0.001323\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.0124)\n",
            "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (0.9472)\n",
            "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (0.9930)\n",
            "Entrada: [1 1], Saída Esperada: 1, Previsão da Rede: (1.0412)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.c aproximação da função XNOR"
      ],
      "metadata": {
        "id": "kPdVo6yT78Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epocas = 2000\n",
        "taxaAprendizado = 0.01\n",
        "entradas = np.array([0, 0]).reshape(1,2)\n",
        "np.random.seed(80) # seed para os pesos e vieses\n",
        "\n",
        "#Hiperparâmetros iniciais\n",
        "pesosIniciais = heInitialization(2, 2)\n",
        "pesosFinais = heInitialization(2, 1)\n",
        "viesesIniciais = np.full((1,2),0.1)\n",
        "viesFinal = np.full((1,1),0.1)\n",
        "entradasV = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "saidasV = np.array([[1], [0], [0], [1]])\n",
        "\n",
        "for i in range(epocas):\n",
        "    loss_total = 0\n",
        "\n",
        "    for x_entrada, y_saida in zip(entradasV, saidasV):\n",
        "\n",
        "        entrada = x_entrada.reshape(1,2)\n",
        "        saidaCorreta = y_saida\n",
        "\n",
        "        #calcula a saida da camada oculta\n",
        "        saidaOcultaLinear = entrada @ pesosIniciais\n",
        "        saidaOcultaLinear = saidaOcultaLinear + viesesIniciais\n",
        "        saidaOculta = Relu(saidaOcultaLinear)\n",
        "\n",
        "\n",
        "        #calcula a saida final\n",
        "        saidaFinal = saidaOculta @ pesosFinais\n",
        "        saidaFinal = saidaFinal + viesFinal\n",
        "\n",
        "        #calcula a loss e os gradientes\n",
        "        loss = calculoLoss(saidaFinal, saidaCorreta)\n",
        "        loss_total += loss\n",
        "        gradientes = backWardPass(entrada, saidaCorreta, saidaFinal, pesosFinais, saidaOculta, saidaOcultaLinear)\n",
        "\n",
        "        #define novos parâmetros\n",
        "        pesosFinais = calculoParametro(pesosFinais, taxaAprendizado, gradientes[0]) #arrumar\n",
        "        viesFinal = calculoParametro(viesFinal,taxaAprendizado, gradientes[1])\n",
        "        pesosIniciais = calculoParametro(pesosIniciais, taxaAprendizado, gradientes[2])\n",
        "        viesesIniciais = calculoParametro(viesesIniciais, taxaAprendizado, gradientes[3])\n",
        "\n",
        "    if(i + 1) % 500 == 0:\n",
        "        loss_medio = loss_total / len(entradasV)\n",
        "        print(f\"Época: {i + 1}, loss médio: {loss_medio.item():.6f}\")\n",
        "\n",
        "print(\"\\n--- Testando a Rede Treinada ---\")\n",
        "for x_exemplo, y_exemplo in zip(entradasV, saidasV):\n",
        "    entrada = x_exemplo.reshape(1, 2)\n",
        "    saidaOcultaLinear = entrada @ pesosIniciais + viesesIniciais\n",
        "    saidaOculta = Relu(saidaOcultaLinear)\n",
        "    saidaFinal = saidaOculta @ pesosFinais + viesFinal\n",
        "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
        "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo[0]}, Previsão da Rede: ({saidaFinal[0][0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwK_j8Xy8zbx",
        "outputId": "3023f2b7-3b1b-44c9-f5ce-7f09f933c2f7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 500, loss médio: 0.022581\n",
            "Época: 1000, loss médio: 0.000007\n",
            "Época: 1500, loss médio: 0.000000\n",
            "Época: 2000, loss médio: 0.000000\n",
            "\n",
            "--- Testando a Rede Treinada ---\n",
            "Entrada: [0 0], Saída Esperada: 1, Previsão da Rede: (1.0000)\n",
            "Entrada: [0 1], Saída Esperada: 0, Previsão da Rede: (-0.0000)\n",
            "Entrada: [1 0], Saída Esperada: 0, Previsão da Rede: (0.0000)\n",
            "Entrada: [1 1], Saída Esperada: 1, Previsão da Rede: (1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliando a aplicação da arquitetura padrão para aproximar outras funções binárias, fica evidente a versatilidade e precisão da rede. As funções AND e XNOR tiveram uma queda muito rápida na loss, aproximando-as perdeitamente em 2000 épocas, enquanto a função OR não conseguiu uma aproximação perfeita no mesmo tempo."
      ],
      "metadata": {
        "id": "bEG3iVeu82wZ"
      }
    }
  ]
}