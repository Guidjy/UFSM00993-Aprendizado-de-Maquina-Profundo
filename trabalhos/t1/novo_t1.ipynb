{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24edd331",
   "metadata": {},
   "source": [
    "1. Utilizando Python e Numpy, implemente uma Multi-layer Perceptron (MLP) com uma camada oculta de largura 2 (i.e., uma Perceptron com duas unidades ocultas), que aproxima função “ou exclusivo” (XOR) definida como XOR : {0, 1}² → {0, 1}.\n",
    "<img src=\"mlp_xor.png\"><img/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2a49ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952efca",
   "metadata": {},
   "source": [
    "2. Considere que apenas a camada oculta tem uma função de ativação (adote ReLU) (i.e., a camada de saída tem\n",
    "ativação identidade). Otimize a MLP com uma função de perda que penaliza o quadrado do erro de estimação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0393dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def I(x):\n",
    "    return x\n",
    "\n",
    "def funcao_perda(saida_esperada, saida_real):\n",
    "    return (saida_real - saida_esperada) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f043fc",
   "metadata": {},
   "source": [
    "3. Inicialize os coeficientes das matrizes de pesos Ω de tamanho N × M com os valores Ωij = N(0, √(2/N)) (He initialization). Inicialize os coeficientes dos vetores de viéses β com um valor constante pequeno (e.g., βk = 0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3ff3000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def He_initialization(n, m):\n",
    "    pesos = np.random.randn(n, m) * np.sqrt(2 / n)\n",
    "    return pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92329449",
   "metadata": {},
   "source": [
    "4. Considere o método do gradiente descendente para otimização. Determine as derivadas da função de perda em função de cada parâmetro e as implemente manualmente. Identifique os gradientes textualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a9f7a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(entradas, saida_correta, saida_aproximada, saida_pesos, oculta_saida, oculta_saida_linear):\n",
    "    \"\"\"\n",
    "    Otimiza os parâmetros da MLP usando o método do gradiente descendente.\n",
    "    >argumentos:\n",
    "    - entradas = vetor de entradas\n",
    "    - saida_correta = saida correta esperada\n",
    "    - saida_aproximada = saida aproximada pela MLP\n",
    "    - saida_pesos = matriz de pesos da camada de saída\n",
    "    - oculta_saida: saida da oculta depois de ser aplicada a uma função de ativação\n",
    "    - oculta_saida_linear: saida da oculta antes de ser aplicada a uma função de ativação\n",
    "    >retorno: [gradiente_vies_saida, gradiente_peso_saida, gradiente_vies_oculta, gradiente_peso_oculta]\n",
    "    \"\"\"\n",
    "\n",
    "    # gradiente do viés da camada de saída\n",
    "    # é a derivada da função de perda em relação aos pesos da camada de saída\n",
    "    gradiente_vies_saida = 2 * (saida_aproximada - saida_correta)\n",
    "    \n",
    "    # gradiente dos pesos da camada de saída\n",
    "    # é o produto  entre as ativações da camada oculta e o erro da camada de saída\n",
    "    gradiente_peso_saida = oculta_saida.T @ gradiente_vies_saida\n",
    "    \n",
    "    # gradiente dos viéses da camada oculta\n",
    "    # Erro propagado para a camada oculta.\n",
    "    derivada_ReLU = (oculta_saida_linear > 0).astype(int)\n",
    "    gradiente_vies_oculta = (gradiente_vies_saida @ saida_pesos.T) * derivada_ReLU\n",
    "    \n",
    "    # gradiente dos pesos da camada oculta\n",
    "    # erro propagado para a camada oculta\n",
    "    gradiente_pesos_oculta = entradas.T @ gradiente_vies_oculta\n",
    "    \n",
    "    return gradiente_vies_saida, gradiente_peso_saida, gradiente_vies_oculta, gradiente_pesos_oculta\n",
    "    \n",
    "    \n",
    "def atualiza_parametro(parametro_antigo, taxa_aprendizado, gradiente):\n",
    "    return parametro_antigo - taxa_aprendizado * gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e037d",
   "metadata": {},
   "source": [
    "5. Avalie experimentalmente o impacto de retreinar a MLP. Por fim, determine uma seed para os experimentos seguintes. Descreva os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e4642dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época: 500, loss médio: 0.151750\n",
      "Época: 1000, loss médio: 0.103305\n",
      "Época: 1500, loss médio: 0.007572\n",
      "Época: 2000, loss médio: 0.000007\n",
      "Época: 2500, loss médio: 0.000000\n",
      "Época: 3000, loss médio: 0.000000\n",
      "\n",
      "--- Testando a Rede Treinada ---\n",
      "Entrada: [0 0], Saída Esperada: 0, Previsão da Rede: (0.0000)\n",
      "Entrada: [0 1], Saída Esperada: 1, Previsão da Rede: (1.0000)\n",
      "Entrada: [1 0], Saída Esperada: 1, Previsão da Rede: (1.0000)\n",
      "Entrada: [1 1], Saída Esperada: 0, Previsão da Rede: (0.0000)\n"
     ]
    }
   ],
   "source": [
    "# hiperparâmetros\n",
    "epocas = 3000\n",
    "taxa_aprendizado = 0.01\n",
    "np.random.seed(80)\n",
    "\n",
    "# pesos e viéses da camada oculta\n",
    "h_pesos = He_initialization(2, 2)\n",
    "h_vieses = np.array([[0.1, 0.1]])\n",
    "\n",
    "# pesos e viéses da camada de saída\n",
    "yhat_pesos = He_initialization(2, 1)\n",
    "yhat_vieses = np.array([[0.1]])\n",
    "\n",
    "# entradas e saídas esperadas da MLP\n",
    "entradas = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "saidas_esperadas = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "# machine.learn() \n",
    "for i in range(epocas):\n",
    "    perda_total = 0\n",
    "    \n",
    "    for entrada_x, saida_y in zip(entradas, saidas_esperadas):    \n",
    "        entrada_x = entrada_x.reshape(1, -1)    # (1, 2)\n",
    "        saida_y = np.array([[saida_y]])         # (1, 1)\n",
    "\n",
    "            \n",
    "        # calcula a saida da camada oculta\n",
    "        saida_oculta_linear = entrada_x @ h_pesos\n",
    "        saida_oculta_linear += h_vieses\n",
    "        saida_oculta = np.vectorize(ReLU)(saida_oculta_linear)\n",
    "        \n",
    "        # calcula a saida da camada de saida\n",
    "        saida = saida_oculta @ yhat_pesos + yhat_vieses\n",
    "\n",
    "        \n",
    "        #calcula a perda\n",
    "        perda = funcao_perda(saida_y, saida)\n",
    "        perda_total += perda\n",
    "        \n",
    "        # calcula os gradientes\n",
    "        gradientes = backward_pass(entrada_x, saida_y, saida, yhat_pesos, saida_oculta, saida_oculta_linear)\n",
    "        \n",
    "        # atualiza os parâmetros\n",
    "        yhat_vieses = atualiza_parametro(yhat_vieses, taxa_aprendizado, gradientes[0])\n",
    "        yhat_pesos = atualiza_parametro(yhat_pesos, taxa_aprendizado, gradientes[1])\n",
    "        h_vieses = atualiza_parametro(h_vieses, taxa_aprendizado, gradientes[2])\n",
    "        h_pesos = atualiza_parametro(h_pesos, taxa_aprendizado, gradientes[3])\n",
    "    \n",
    "    if(i + 1) % 500 == 0:\n",
    "        perda_medio = perda_total / len(entradas)\n",
    "        print(f\"Época: {i + 1}, loss médio: {perda_medio.item():.6f}\")\n",
    "\n",
    "print(\"\\n--- Testando a Rede Treinada ---\")\n",
    "for x_exemplo, y_exemplo in zip(entradas, saidas_esperadas):\n",
    "    entrada = x_exemplo.reshape(1, 2)\n",
    "    saida_oculta_linear = entrada @ h_pesos + h_vieses\n",
    "    saida_oculta = np.vectorize(ReLU)(saida_oculta_linear)\n",
    "    saida = saida_oculta @ yhat_pesos + yhat_vieses\n",
    "    # Arredonda a previsão para 0 ou 1 para uma comparação clara\n",
    "    print(f\"Entrada: {x_exemplo}, Saída Esperada: {y_exemplo}, Previsão da Rede: ({saida[0][0]:.4f})\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
